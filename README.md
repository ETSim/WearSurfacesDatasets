# Wear Surfaces Dataset Pipeline

A suite of Python CLIs for processing, converting and managing highâ€‘resolution surfaceâ€‘texture datasets captured under controlled wear tests.

---

## ğŸ” Key Features

- **Perâ€‘Leaf Metadata** (`generate_metadata.py`)  
  â€¢ Infer `material`, `shape`, `direction`, `grit`, `load_g`, `distance_mm`, `replicate` from folder structure  
  â€¢ Record file sizeÂ & dimensions for each map (`ao.png`, `bump.png`, â€¦)  
  â€¢ Emit one `metadata.json` per leaf and an aggregated `index.json`

- **Parquet Conversion** (`generate_datasets.py parquet`)  
  â€¢ Scan your `wear/` tree of PNG â€œmapsâ€  
  â€¢ Compress each PNG to **JPEG (quality=50)** inâ€‘memory  
  â€¢ Bundle perâ€‘sample groups into Snappyâ€‘compressed Parquet shards  
  â€¢ Produce a topâ€‘level `metadata.parquet` index

- **WebDataset Shards** (`generate_datasets.py webdataset`)  
  â€¢ Reâ€‘encode JPEGÂ +Â JSON metadata  
  â€¢ Shard into `.tar` archives for efficient streaming

- **Image Extraction & Decompression**  
  â€¢ `extract` (Parquet â†’ PNG)  
  â€¢ `decompress` (WebDatasetÂ .tar â†’ PNG)

- **Smartâ€‘Update & Full Pipeline**  
  â€¢ **smartâ€‘update**: only reprocess changed files (SHA256â€‘based)  
  â€¢ **runâ€‘all**: endâ€‘toâ€‘end rebuild (wear â†’ Parquet â†’ shards)

---

## ğŸ“‚ Repository Layout

```
wear/                       # Raw dataset root
â”œâ”€â”€ Cartboard/              # â€¦ leaf dirs with maps + metadata.json
â””â”€â”€ Soft_Wood/

data/                       # Outputs
â”œâ”€â”€ parquet/           # Parquet shards + metadata.parquet
â”œâ”€â”€ webdataset/      # TAR shards of JPEG+JSON
â”œâ”€â”€ images/       # PNGs reconstructed 

â”œâ”€â”€ generate_metadata.py    # leaf / global / index metadata
â”œâ”€â”€ generate_datasets.py    # parquet / webdataset / extract / decompress / smart-update / run-all
â”œâ”€â”€ read_datasets.py        # parquetâ€only extract & shard decompress helpers

LICENSE                     # MIT license
README.md                   # This document
```

---

## âš™ï¸ Installation

Requires PythonÂ 3.7+ and:

```bash
pip install pandas numpy pillow rich pyarrow typer
```

---

## ğŸš€ Usage

### A. Metadata

#### 1. Perâ€‘leaf metadata

```bash
python3 scripts/generate_metadata.py leaf wear/ \
  [--dry-run] [--verbose]
```

Writes a `metadata.json` in each leaf directory under `wear/`.

#### 2. Global metadata

```bash
python3 scripts/generate_metadata.py global_ wear/ \
  --template global_template.json \
  [--dry-run]
```

Loads a JSON template, updates `datasetInfo.lastUpdated`, writes topâ€‘level `wear/metadata.json`.

#### 3. Aggregate index

```bash
python3 scripts/generate_metadata.py index wear/ \
  [--output all_leaf_index.json] [--dry-run]
```

Scans every leafâ€™s `metadata.json` and writes a combined `index.json`.

---

### B. Data Conversion & Sharding

All commands assume your raw PNGs live in `wear/`.

#### 1. Parquet Conversion

```bash
python3 scripts/generate_datasets.py parquet \
  --input-dir wear \
  --output-dir data/parquet_data \
  --workers 8
```

- Compresses each PNG â†’ JPEG (quality=50)  
- Writes perâ€‘sample Parquet:  
  `data/parquet_data/<Material>/<Shape>/<Direction>/S<grit>/<load>g/<distance>mm/[rep<â€¦>.parquet|data.parquet]`  
- Creates `data/parquet_data/metadata.parquet`

#### 2. WebDataset Shards

```bash
python3 generate_datasets.py webdataset \
  --input-dir wear \
  --output-dir data/webdataset_shards \
  --shard-size 100 \
  --workers 8
```

Encodes JPEGÂ + JSON and shards into `.tar` files under `data/shards/`.


#### Smartâ€‘Update & Full Pipeline

- **Smartâ€‘update** (only changed files):

  ```bash
  python generate_datasets.py smart-update \
    --wear-dir wear \
    --parquet-dir data/parquet_data \
    --shards-dir data/webdataset_shards \
    --shard-size 100 \
    --workers 8
  ```

- **Full rebuild**:

  ```bash
  python scripts/generate_datasets.py run-all \
    --wear-dir wear \
    --parquet-dir data/parquet_data \
    --shards-dir data/webdataset_shards \
    --shard-size 100 \
    --workers 8
  ```

---

### C. Reconstruction

#### 3. Extract from Parquet

```bash
python3 read_datasets.py extract \
  --parquet-dir data/parquet \
  --output-dir data/images \
  [--material Cartboard] [--shape Circle] [--direction Linear] \
  [--grit 60] [--weight 100] [--distance 1200] [--workers 8]
```

Rebuilds the original PNG maps into the same folder hierarchy under `data/images/`.

#### 4. Decompress WebDataset

```bash
python3 read_datasets.py decompress \
  --shards-dir data/shards \
  --output-dir data/images \
  --workers 4
```

Unpacks each `.tar` back into `data/images/...`.


---

## ğŸŒ² Example `wear/` Structure

```
wear/
â””â”€â”€ Cartboard/
    â””â”€â”€ Circle/
        â””â”€â”€ Linear/
            â””â”€â”€ S60/
                â”œâ”€â”€ 100g/0mm/       â† ao.png, bump.png, â€¦, metadata.json
                â”œâ”€â”€ 100g/1200mm/
                â””â”€â”€ 100g/2400mm/2/  â† replicate #2
```

---

## ğŸ“„ License

Released under the **MIT License**. See [LICENSE](LICENSE) for details.
